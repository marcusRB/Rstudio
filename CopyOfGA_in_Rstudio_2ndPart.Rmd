---
title: "Google Analytics and Machine Learning 2nd Part"
author: "Marco Russo"
date: "31 de enero de 2019"
output: html_document
---

```{r message=FALSE}
# Instalamos las librerias
library("forecast")
library("reshape2")
library("stats")
library("ggplot2")
library("ggthemes")
library("ggrepel")
library(dplyr)
library(tidyr)

```


## Create dataframe with main metrics and dimensions


```{r include=FALSE}
# Creamos dataframe con diferentes variables
web_data_metrics <- google_analytics_3(id = ga_id,
                                       start="2018-01-01", end="2018-06-30",
                                       metrics = c("sessions","pageviews","entrances","bounces"),
                                       dimensions = c("date","channelGrouping","deviceCategory"),
                                       max = 5000,
                                       samplingLevel = 'WALK')

```

```{r}
## get only desktop rows, and the date, channelGrouping and sessions columns
pivoted <- web_data_metrics %>% 
  filter(deviceCategory == "desktop") %>% 
  select(date, channelGrouping, sessions) %>%
  spread(channelGrouping, sessions)

## get rid of any NA's and replace with 0
pivoted[is.na(pivoted)] <- 0

head(pivoted)
```


## A seasonal time-series


```{r}
## make a time-series object
web_data_ts <- ts(pivoted[-1], frequency = 7)

## time-series are set up to have useful plots
plot(web_data_ts, axes = FALSE)

```


### Time-series appplications
Decomposition
With seasonal data, you can extract the seasonality (weekly in this example) so you can more easily see the overall trend of data:
```{r}
decomp <- decompose(web_data_ts[, "Organic Search"])
plot(decomp)
```

- Observed - the original data
- Trend - the data minus the seasonal
- Seasonal - data that fits the specified seasonality (7 days)
- Random - everything else left only - useful for anomoloy detection.



```{r}
library(forecast)
## performs decomposition and smoothing
fit <- ets(web_data_ts[, "Organic Search"])
## makes the forecast
fc <- forecast(fit)
plot(fc)

```



## From Holtwinters
```{r}

fit2 <- HoltWinters(web_data_ts[, "Organic Search"])
## makes the forecast
fc2 <- forecast(fit2, h = 25)
plot(fc2)

```





# Forecasting 26 días

```{r message=FALSE}
# 
gadata_5 <- google_analytics_3(id = ga_id, 
                           start="2018-06-01", end="2018-06-30",
                           metrics = "sessions", 
                           dimensions = "date",
                           max = 5000)


timeseries <- ts(gadata_5$sessions, frequency=7)
components <- decompose(timeseries)
plot(components)

# note the way we add a column to a data.frame
gadata_5$adjusted <- gadata_5$sessions - components$seasonal

theme(axis.text.x = element_text(angle = 90, hjust = 1))

forecastmodel <- HoltWinters(timeseries)
plot(forecastmodel)

forecast <- forecast:::forecast.HoltWinters(forecastmodel, h=26) # 26 days in future
plot(forecast, xlim=c(0,12))

forecastdf <- as.data.frame(forecast)
totalrows <- nrow(gadata_5) + nrow(forecastdf)
forecastdata <- data.frame(day=c(1:totalrows),
actual=c(gadata_5$sessions,rep(NA,nrow(forecastdf))),
forecast=c(rep(NA,nrow(gadata_5)-1),tail(gadata_5$sessions,1),forecastdf$"Point Forecast"),
forecastupper=c(rep(NA,nrow(gadata_5)-1),tail(gadata_5$sessions,1),forecastdf$"Hi 80"),
forecastlower=c(rep(NA,nrow(gadata_5)-1),tail(gadata_5$sessions,1),forecastdf$"Lo 80")
)

ggplot(forecastdata, aes(x=day)) +
geom_line(aes(y=actual),color="black") +
geom_line(aes(y=forecast),color="blue") +
geom_ribbon(aes(ymin=forecastlower,ymax=forecastupper), alpha=0.4, fill="green") +
xlim(c(0,90)) +
xlab("Day") +
ylab("Sessions")
```

# Forecasting 90 días

```{r message=FALSE}
library("forecast")
library("reshape2")
library("stats")

gadata_6 <- google_analytics_3(id = ga_id, 
                           start="2019-01-01", end="2019-01-30",
                           metrics = "sessions", 
                           dimensions = "date",
                           max = 5000)


timeseries <- ts(gadata_6$sessions, frequency=7)
components <- decompose(timeseries)
plot(components)

# note the way we add a column to a data.frame
gadata_6$adjusted <- gadata_6$sessions - components$seasonal

theme(axis.text.x = element_text(angle = 90, hjust = 1))

forecastmodel <- HoltWinters(timeseries)
plot(forecastmodel)

forecast <- forecast:::forecast.HoltWinters(forecastmodel, h=30) # 26 days in future
plot(forecast, xlim=c(0,13))

forecastdf <- as.data.frame(forecast)
totalrows <- nrow(gadata_6) + nrow(forecastdf)
forecastdata <- data.frame(day=c(1:totalrows),
actual=c(gadata_6$sessions,rep(NA,nrow(forecastdf))),
forecast=c(rep(NA,nrow(gadata_6)-1),tail(gadata_6$sessions,1),forecastdf$"Point Forecast"),
forecastupper=c(rep(NA,nrow(gadata_6)-1),tail(gadata_6$sessions,1),forecastdf$"Hi 50"),
forecastlower=c(rep(NA,nrow(gadata_6)-1),tail(gadata_6$sessions,1),forecastdf$"Lo 50")
)

ggplot(forecastdata, aes(x=day)) +
geom_line(aes(y=actual),color="black") +
geom_line(aes(y=forecast),color="blue") +
geom_ribbon(aes(ymin=forecastlower,ymax=forecastupper), alpha=0.4, fill="green") +
xlim(c(0,50)) +
xlab("Day") +
ylab("Sessions")
```


### Converting to xts for some packages
We can only work with numeric data, but we would also like to keep the date information, mainly for labelling purposes. xts lets you create the time-series whilst specifying the date labels like so:

```{r}
library(xts)

## create a time-series zoo object
web_data_xts <- xts(pivoted[-1], order.by = as.Date(pivoted$date), frequency = 7)

```

web_data_xts will look similar to when it was a data.frame, but its now in the right class to do some nice related functions.

### CausalImpact
A good example of a library that needs xts is CausalImpact, which is a fantastic library from Google well suited for digital marketing.

CausalImpact gives you an estimate on how much effect an event at a certain point of time had on your metrics, in absolute and relative terms. In this case, an event could be a TV campaign starting or changing your Title tags. It also lets you add control segments, so you can adjust for known effects.

As an example, we assume the observed peak in Social traffic is due to the start of some campaign in May 15th 2016 (in reality you should not cherry pick dates like this!) and we woud like to observe its effect on Video sessions. We also add Direct sessions as a control to help account for general website trends.


```{r}
# install.packages(CausalImpact)
library(CausalImpact)
library(bupaR)
pre.period <- as.Date(c("2018-03-13","2018-04-14"))
post.period <- as.Date(c("2018-04-15","2018-06-30"))

## data in order of response, predictor1, predictor2, etc.
model_data <- web_data_xts[,c("Direct","Social","Organic Search","Paid Search","Referral")]


impact <- CausalImpact(model_data,  pre.period, post.period)
plot(impact)


```

- original - the original response data, with a forecast (blue) of where it should have been in post event period
- pointwise - the difference between the forecast and reality per day
- cumulative - the cumuative effect on sessions in the post event period




```{r message=FALSE}
gadata_7 <- google_analytics_3(id = ga_id, 
                           start="2018-01-01", end="2018-06-30",
                           metrics = c("sessions",
                                       "users",
                                       "transactions",
                                       "transactionRevenue"
                                       ),
                           dimensions = c("channelGrouping"),   # pull the default channel grouping as the only dimension
                           #anti_sample = TRUE,    # if you happen to encounter sampling, add this parameter to pull data in multiple batches, automatically
                                      
                           max = 5000,
                           )

gadata_7
```



```{r}
# now let's make some calculations on the sessions/users share and conversion rates
sources_clean = gadata_7 %>%
  mutate(
    session_share = sessions / sum(sessions),
    sales_share = transactions / sum(transactions),
    revenue_share = transactionRevenue / sum(transactionRevenue)
  ) %>%
  arrange(-session_share) %>%
  transmute(
    channel = channelGrouping,
    sessions,
    users,
    sales = transactions,
    revenue = transactionRevenue,
    session_share,
    session_addup = cumsum(session_share),
    sales_share,
    sales_addup = cumsum(sales_share),
    revenue_share,
    revenue_addup = cumsum(revenue_share),
    cr_sessions = transactions / sessions,
    cr_users = transactions / users,
    rps = revenue / sessions,
    rpu = revenue / users
  )
sources_clean
```



```{r}
sources_clean %>%         # passing our data frame into the plot function
  ggplot(
    aes(                  # specifying which fields should we use for plotting
      x = session_share,
      y = sales_share,
      color = channel
    )
  ) +
  geom_point(alpha = 5/7) # specifying the type of the plot we want
```



```{r}
# 
sources_clean %>%
  filter(sales >= 10) %>%   # show only the channels with 10+ sales
  ggplot(
    aes(
      x = session_share,
      y = sales_share,
      color = channel
    )
  ) +
  geom_abline(slope = 1, alpha = 1/10) +
  geom_point(alpha = 5/7) +
  theme_minimal(base_family = "Helvetica Neue") +
  theme(legend.position = "none") +
  scale_x_continuous(name = "Share of sessions", limits = c(0, NA), labels = "%") +
  scale_y_continuous(name = "Share of sales", limits = c(0, NA), labels = "%") +
  scale_color_few(name = "Channel") +
  scale_fill_few() +
  ggtitle(
    "Sessions and sales distribution for top channels",
    subtitle = "Based on the Google Analytics data"
  ) +
  geom_label_repel(alpha = 1/2, aes(label = channel), show.legend = F)
```




```{r}
# I use the table "query_results" with my analytics data
# run it from a query or run:
# query_results <- read.csv("./query-results.csv")
 
log <- query_results %>%
  eventlog(
    case_id = "case_id",                           # the user id (browser cookie)
    activity_id = "activity_id",                   # this contains the page name that is viewd
    activity_instance_id = "activity_instance_id", # all user activity from that page view
    lifecycle_id = "lifecycle_id",                 # page activity: one of 'start' or 'end' per page (funnel-step)
    timestamp = "timestamp",
    resource_id = "resource_id"                    # I fill this with device_type
  )
```


## Correlation


```{r}
## see correlation between all metrics
kable(cor(web_data_metrics[,c("sessions","pageviews","entrances","bounces")]))
```


```{r}
## see correlation between all metrics
pairs(web_data_metrics[,c("sessions","pageviews","entrances","bounces")])
```


```{r}
## see correlation between all metrics
cor(web_data_metrics[,c("sessions","pageviews","entrances","bounces")])
```


## How do web channels correlate?
One useful piece of analysis is seeing how web channels possibly interact

```{r}
## Use tidyverse to pivot the data
library(dplyr)
library(tidyr)
library(knitr)

# Create un nuevo dataset
gadata_8 <- google_analytics_3(id = ga_id, 
                           start="2018-01-01", end="2018-06-30",
                           metrics = "sessions", 
                           dimensions = c("date","deviceCategory","channelGrouping"),
                           max = 5000)



## Get only desktop rows, and the date, channelGrouping and sessions columns
pivoted <- web_data_metrics %>% 
  filter(deviceCategory == "desktop") %>% 
  dplyr::select(date, channelGrouping, sessions) %>%
  spread(channelGrouping, sessions)

## Get rid of any NA's and replace with 0
pivoted[is.na(pivoted)] <- 0

kable(head(pivoted))


```


```{r}
## can't include the date as its not numeric, so remove
cor_data <- pivoted[, -1]
## not including first column, so -1 subset
cor_table <- round(cor(cor_data),2)
kable(cor_table)

```

```{r}
## can't include the date as its not numeric, so remove
pairs(cor_data)

```

## Analysis

Now, when we compare channels, we see much looser correlations for this dataset, which makes sense, right? Correlations under 0.3 are, as a rule-of-thumb, not worth considering, so the standouts look to be Social vs. Video* and Paid** vs. Organic Search.

Plotting those channels, we can examine the trends to see the shape of the data

Correlation has help us zero in on possibly interesting relationships


```{r}
library(ggplot2)
gg <- ggplot(data = pivoted) + 
      theme_minimal() + 
      ggtitle("Pago (blue) vs Orgánico (green)")
gg <- gg + 
      geom_line(aes(x = as.Date(date), y = `Paid Search`), col = "blue")

gg + geom_line(aes(x = as.Date(date), y = `Organic Search`), col = "green")

``` 

```{r}
library(ggplot2)
gg <- ggplot(data = pivoted) + 
              theme_minimal() + 
              ggtitle("Social (red) vs Referral (orange)")
gg <- gg + 
      geom_line(aes(x = as.Date(date), y = Social), col = "red")
gg + geom_line(aes(x = as.Date(date), y = Referral), col = "orange")

```


```{r}
library(ggplot2)
gg <- ggplot(data = pivoted) + 
              theme_minimal() + 
              ggtitle("Orgánico(verde) vs Referral (orange)")
gg <- gg + 
      geom_line(aes(x = as.Date(date), y = `Organic Search`), col = "green")
gg + geom_line(aes(x = as.Date(date), y = Referral), col = "orange")

```


```{r}
library(ggplot2)
gg <- ggplot(data = pivoted) + 
              theme_minimal() + 
              ggtitle("Directo (gris) vs Paid (blue)")
gg <- gg + 
      geom_line(aes(x = as.Date(date), y = `Direct`), col = "grey")
gg + geom_line(aes(x = as.Date(date), y = `Paid Search`), col = "blue")

```


```{r}
library(ggplot2)
gg <- ggplot(data = pivoted) + 
              theme_minimal() + 
              ggtitle("Directo (gris) vs Display (blue)")
gg <- gg + 
      geom_line(aes(x = as.Date(date), y = `Direct`), col = "grey")
gg + geom_line(aes(x = as.Date(date), y = `Display`), col = "blue")

```





```{r}
ccf(pivoted$Social, pivoted$Referral)

``` 

```{r}
library(dplyr) # data transformation

head(web_data_metrics)

cats <- web_data_metrics %>% dplyr::select(deviceCategory, channelGrouping, sessions)


## Get only desktop rows, and the date, channelGrouping and sessions columns
pivoted <- gadata_8 %>% 
  filter(deviceCategory == "desktop") %>% 
  dplyr::select(date, channelGrouping, sessions) %>%
  spread(channelGrouping, sessions)

## Get rid of any NA's and replace with 0
pivoted[is.na(pivoted)] <- 0

kable(head(pivoted))



kable(head(cats), row.names = FALSE)

```


```{r}
library(dplyr) # data transformation
cats <- web_data_metrics %>% dplyr::select(deviceCategory, channelGrouping, sessions)

kable(head(cats), row.names = FALSE)

```

```{r}
library(rpart) # creates decision trees

tree <- rpart(deviceCategory ~ ., cats)
plot(tree)
text(tree)
```




```{r}
library(rpart.plot)  # pretty trees
rpart.plot(tree, type=2) 

```











